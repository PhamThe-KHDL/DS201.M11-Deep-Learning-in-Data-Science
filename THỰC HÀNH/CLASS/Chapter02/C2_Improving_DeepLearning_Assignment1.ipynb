{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Đây là bài tập đầu tiên trong chuỗi các bài về cải thiện Neural Network.\n",
    "\n",
    "Training một neural network đòi hỏi chúng ta phải khởi tạo các tham số W và b cho tất cả layer. Ở các bài trước, ta khởi tạo W ngẫu nhiên và nhân W với 0.01 để đảm bảo W nhỏ, và b khởi tạo bằng zero. Với logistic regression và neural network với 1 hidden layer thì cách khởi tạo này hoạt động được (mạng có thể học được). Nhưng khi thử với mạng L layer thì mạng lại không học được (cost không giảm sau các lần lặp). \n",
    "\n",
    "Trong bài tập này, chúng ta sẽ thử qua các cách khởi tạo khác nhau và học cách khởi tạo hợp lý cho neural network.\n",
    "\n",
    "\n",
    "Khởi tạo tham số một cách hợp lý giúp\n",
    "- Tăng tốc độ hội tụ của gradient descent\n",
    "- Tăng cơ hội gradient descent hội tụ về một mức lỗi nhỏ hơn ở tập train \n",
    "- Làm giảm đáng kể hiện tượng vanishing và exploding gradient\n",
    "\n",
    "Để bắt đầu, hãy load dataset \"circle\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from c2_a1_nit_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\n",
    "from c2_a1_nit_utils import update_parameters, predict, load_dataset, load_cat_dataset, plot_decision_boundary, predict_dec\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# load image dataset: blue/red dots in circles\n",
    "train_X, train_Y, test_X, test_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn cần train Neural Network để phân loại 2 lớp xanh và đỏ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Neural Network model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn sẽ dùng một neural network với 3 lớp (được cài đặt tương tự Neural Network L layer). Ở đây bạn sẽ thử nghiệm 3 phương pháp khởi tạo tham số:\n",
    " \n",
    "- *Zeros initialization* --  khởi tạo tham số toàn zero.\n",
    "- *Random initialization* -- khởi tạo tham số random.\n",
    "- *He initialization* -- khởi tạo tham số ngẫu nhiên nhưng sau đó scale lại theo một factor đề xuất bởi He et al., 2015. \n",
    "\n",
    "Dưới đây là mô hình Neural Network của chúng ta. Ngoài các biến đầu vào quen thuộc thì ta có một biến \"init_method\" là một chuỗi dùng để chỉ định 1 trong 3 phương pháp khởi tạo đề cập ở trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.01, num_iterations = 15000, print_cost = True, init_method = \"he\"):\n",
    "\n",
    "        \n",
    "    grads = {}\n",
    "    costs = [] # to keep track of the loss\n",
    "    m = X.shape[1] # number of examples\n",
    "    \n",
    "    \n",
    "    # dùng phương pháp khởi tạo chỉ định bởi biến init_method\n",
    "    if init_method == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif init_method == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif init_method == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Loss\n",
    "        cost = compute_loss(a3, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the loss\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Zero initialization\n",
    "\n",
    "Bài tập 1:\n",
    "\n",
    "Khởi tạo ma trận W và vector cột b cho tất cả L layer. Hãy chú ý số chiều của W và b khớp với kích thước của các layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_zeros \n",
    "\n",
    "def initialize_parameters_zeros(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = \n",
    "        parameters['b' + str(l)] = \n",
    "        ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_zeros([3,2,1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.  0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thử train mô hình với 15,000 lần lặp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layers_dims = [train_X.shape[0], 10, 5, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, init_method = \"zeros\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình có độ chính xác 50%, tức là ngang với việc chọn bằng cách tung đồng xu. Hãy xem thử mô hình sẽ đưa ra nhãn dự đoán thế nào cho tất cả các điểm trên mặt phẳng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"predictions_train = \" + str(predictions_train))\n",
    "print (\"predictions_test = \" + str(predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with Zeros initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, np.squeeze(train_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy mô hình dự đoán nhãn 0 cho tất cả mọi điểm. \n",
    "\n",
    "Nguyên nhân cho điều này là khi ta khởi tạo tham số W toàn zeros, mọi neuron ở mỗi layer sẽ học như nhau. Như vậy, neural network của chúng ta sẽ tương đương với một neural network có số neuron bằng 1 ($n^{[l]}=1$) tại mỗi layer. Rõ ràng một neural network như thế thì không thể học được gì. \n",
    "\n",
    "Lưu ý ta chỉ cần khởi tạo W ngẫu nhiên (không phải toàn zero), còn b thì có thể được khởi tạo toàn zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Random initialization\n",
    "\n",
    "Bài tập 2:\n",
    "\n",
    "Như vậy ta cần phải khởi tạo W ngẫu nhiên (không phải toàn zero). Để điều chỉnh độ lớn của W, ta sẽ nhân W cho một biến \"scale\" có giá trị mặc định là 10. Ta có thể thử các \"scale\" khác nhau. Tham số b vẫn được khởi tạo toàn zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_random\n",
    "\n",
    "def initialize_parameters_random(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    scale=10;\n",
    "    #np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as ours\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # integer representing the number of layers\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### \n",
    "        parameters['W' + str(l)] = \n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_random([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 17.88628473   4.36509851   0.96497468]\n",
    " [-18.63492703  -2.77388203  -3.54758979]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.82741481 -6.27000677]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy thử train mô hình với 15,000 lần lặp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layers_dims = [train_X.shape[0], 10, 5, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, init_method = \"random\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn có thể thấy cost giảm sau mỗi lần lặp, điều này chứng tỏ mô hình có thể học ở một mức nhất định. Hãy xem thử mô hình dự đoán nhãn thế nào cho mọi điểm trên mặt phẳng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predictions_train)\n",
    "print (predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with large random initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, np.squeeze(train_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when $\\log(a^{[3]}) = \\log(0)$, the loss goes to infinity.\n",
    "- Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. \n",
    "- If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.\n",
    "\n",
    "<font color='blue'>\n",
    "\n",
    "    \n",
    "Bạn có thể thử với các mức scale khác. Khi giảm scale về 1 mô hình đạt hiệu suất khá cao, nhưng khi giảm scale tiếp về 0.1 thì mô hình lại không học được. Ở phần tiếp theo, ta sẽ học được cách điều chỉnh scale thế nào để mô hình học tốt nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - He initialization\n",
    "\n",
    "Phương pháp khởi tạo này được đặt theo tên của tác giả đề xuất ra nó trong bài báo He et al., 2015. \n",
    "\n",
    "Bài tập 3: Cài đặt hàm khởi tạo theo phương pháp He. Với phương pháp này, ma trận W tại layer $[l]$ sẽ được nhân với một factor $scale=\\sqrt{\\frac{2}{\\text{kích thước layer [l-1]}}}$ (tức là $scale=\\sqrt{\\frac{2}{n^{[l-1]}}}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_he\n",
    "\n",
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        ### START CODE HERE ### \n",
    "        scale =        \n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*scale;\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 1.78862847  0.43650985]\n",
    " [ 0.09649747 -1.8634927 ]\n",
    " [-0.2773882  -0.35475898]\n",
    " [-0.08274148 -0.62700068]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy thử train mô hình với 15,000 lần lặp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers_dims = [train_X.shape[0], 10, 5, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, init_method = \"he\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta thấy cost giảm sau mỗi lần lặp, điều này chứng tỏ mô hình học được. Thực tế ở ví dụ này kích thước các layer không khác nhau nhiều, nên ta có thể thử các mức scale khác nhau và cũng có thể đạt được mức cost tương tự. Ở những mô hình mà kích thước các layer khác nhau nhiều thì ta không thể nào tìm và set một mức scale phù hợp cho mô hình mà không bị hiện tượng vanishing gradient hoặc exploding gradient. \n",
    "\n",
    "Ở Assignment Neural Network L layer, mô hình không học được (cost không giảm sau các lần lặp) là do hiện tượng vanishing gradient. Ta hãy thử lại đúng Neural Network đấy nhưng lần này khởi tạo theo phương pháp \"He\" xem kết quả thế nào nhé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 - Thử lại với tập dữ liệu ảnh mèo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "def load_cat_dataset():\n",
    "    print('here')\n",
    "    train_dataset = h5py.File('Deeplearning/datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    test_dataset = h5py.File('Deeplearning/datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "    test_set_x_flatten =  test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "\n",
    "\n",
    "    train_set_x = train_set_x_flatten/255.\n",
    "    test_set_x = test_set_x_flatten/255.\n",
    "\n",
    "\n",
    "    return train_set_x, train_set_y_orig, test_set_x, test_set_y_orig, classes\n",
    "\n",
    "train_X, train_Y, test_X, test_Y, classes = load_cat_dataset()\n",
    "\n",
    "\n",
    "\n",
    "#layers_dims = [12288, 20, 7, 1] # 4-layer model\n",
    "layers_dims = [train_X.shape[0], 10, 5, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, learning_rate = 0.005, num_iterations = 15000, init_method = \"he\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta thấy mô hình học rất tốt (cost giảm qua các lần lặp và độ chính xác trên tập train là 1.0). Tuy nhiên độ chính xác trên tập test lại không cao, đây là dấu hiệu của overfitting. Các bạn sẽ học được cách giảm overfitting sử dụng các phương pháp regulization ở bài tập sau.\n",
    "\n",
    "Lưu ý: với bài tập này, nếu giảm số lần lặp thì độ chính xác trên tập train giảm, nhưng độ chính xác trên tập test lại tăng (thật ra giảm số lần lặp cũng là một phương pháp regulization gọi là Early Stopping)."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "XOESP",
   "launcher_item_id": "8IhFN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
