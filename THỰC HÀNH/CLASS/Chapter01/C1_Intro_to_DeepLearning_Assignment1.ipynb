{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"XaIWT","launcher_item_id":"zAgPl"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"C1_Intro_to_DeepLearning_Assignment1.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ND4zc_t1eu_n"},"source":["# Logistic Regression ƒë·ªÉ nh·∫≠n d·∫°ng ·∫£nh üê±\n","\n","Trong b√†i t·∫≠p n√†y, c√°c b·∫°n s·∫Ω x√¢y d·ª±ng m·ªôt logistic regression classifier ƒë·ªÉ ph√¢n bi·ªát m·ªôt b·ª©c h√¨nh c√≥ m√®o hay kh√¥ng. C√°c b·∫°n s·∫Ω c√†i ƒë·∫∑t thu·∫≠t to√°n gradient descent ƒë·ªÉ t·ªëi ∆∞u c√°c th√¥ng s·ªë w v√† b c·ªßa logistic regression. \n"]},{"cell_type":"markdown","metadata":{"id":"p-5ytEjseu_1"},"source":["## 1 - Packages ##\n","\n","ƒê·∫ßu ti√™n, h√£y run cell d∆∞·ªõi ƒë·ªÉ import c√°c packages c·∫ßn thi·∫øt."]},{"cell_type":"code","metadata":{"id":"zpC7EtRqeu_4"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy\n","from PIL import Image\n","from scipy import ndimage\n","\n","%matplotlib inline\n","\n","def load_dataset():\n","    train_dataset = h5py.File('Deeplearning/datasets/train_catvnoncat.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # train set labels\n","    test_dataset = h5py.File('Deeplearning/datasets/test_catvnoncat.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # test set labels\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"_nm525BGeu_8"},"source":["## 2 - Nh√¨n nh·∫≠n b√†i to√°n ##\n","\n","C√°c b·∫°n c√≥ m·ªôt dataset (\"data.h5\") ch·ª©a:\n","    - m·ªôt training set ch·ª©a m_train b·ª©c ·∫£nh ƒë∆∞·ª£c d√°n nh√£n l√† cat (y=1) ho·∫∑c non-cat (y=0)\n","    - m·ªôt test set ch·ª©a m_test b·ª©c ·∫£nh cat ho·∫∑c non-cat\n","    - m·ªói b·ª©c ·∫£nh c√≥ k√≠ch th∆∞·ªõc (num_px, num_px, 3) trong ƒë√≥ 3 l√† s·ªë k√™nh (RGB). \n","\n","C√°c b·∫°n s·∫Ω x√¢y d·ª±ng m·ªôt thu·∫≠t to√°n logistic regression ƒë·ªÉ ph√¢n bi·ªát m·ªôt b·ª©c ·∫£nh l√† cat hay non-cat.\n","\n","ƒê·ªÉ l√†m quen v·ªõi dataset, h√£y ch·∫°y cell b√™n d∆∞·ªõi ƒë·ªÉ load d·ªØ li·ªáu."]},{"cell_type":"code","metadata":{"id":"Dj8luVwZeu_9"},"source":["# Loading the data (cat/non-cat)\n","train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hoHXNYIOeu_-"},"source":["M·ªói d√≤ng c·ªßa bi·∫øn train_set_x_orig v√† test_set_x_orig l√† m·ªôt m·∫£ng ch·ª©a gi√° tr·ªã c·ªßa t·∫•t c·∫£ c√°c ƒëi·ªÉm ·∫£nh c·ªßa m·ªôt b·ª©c ·∫£nh. \n","\n","C√°c b·∫°n c√≥ th·ªÉ hi·ªÉn th·ªã m·ªôt b·ª©c ·∫£nh ·ªü v·ªã tr√≠ 'index' v√† xem nh√£n c·ªßa n√≥ b·∫±ng c√°c ch·∫°y cell d∆∞·ªõi ƒë√¢y. "]},{"cell_type":"code","metadata":{"id":"6JaAtNRoevAA"},"source":["# Example of a picture\n","index = 25\n","plt.imshow(train_set_x_orig[index])\n","print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fxgm-Q4yevAB"},"source":["\n","H√£y ch·∫°y cell d∆∞·ªõi ƒë√¢y ƒë·ªÉ t√¨m chi·ªÅu c·ªßa d·ªØ li·ªáu. \n","\n","    - m_train (number of training examples)\n","    - m_test (number of test examples)\n","    - num_px (= height = width of a training image)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"zXhmC_gHevAC"},"source":["m_train = train_set_x_orig.shape[0] \n","m_test = test_set_x_orig.shape[0]\n","num_px = train_set_x_orig.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_g_nImDQevAD"},"source":["Ti·∫øp theo, b·∫°n c·∫ßn l√†m ph·∫≥ng (flatten) c√°c b·ª©c ·∫£nh k√≠ch th∆∞·ªõc (num_px, num_px, 3) th√†nh m·ªôt vector c·ªôt k√≠ch th∆∞·ªõc (num_px $*$ num_px $*$ 3, 1). \n","\n","Sau b∆∞·ªõc n√†y, t·∫≠p train (v√† test) l√† m·ªôt ma tr·∫≠n g·ªìm m_train (v√† m_test) c·ªôt trong ƒë√≥ m·ªói c·ªôt l√† m·ªôt b·ª©c ·∫£nh. \n","\n"]},{"cell_type":"code","metadata":{"id":"pP4LP6-cevAJ"},"source":["train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n","test_set_x_flatten =  test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lkY7IjBEevAK"},"source":["\n","Do gi√° tr·ªã pixel t·ª´ 0 ƒë·∫øn 255, ƒë·ªÉ **chu·∫©n h√≥a d·ªØ li·ªáu**, ta chia t·∫•t c·∫£ gi√° tr·ªã c·ªßa dataset cho 255.\n"]},{"cell_type":"code","metadata":{"id":"HpozLTM8evAK"},"source":["train_set_x = train_set_x_flatten/255.\n","test_set_x = test_set_x_flatten/255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Yc-B_4WevAL"},"source":["\n","## 3 - C·∫•u tr√∫c chung c·ªßa thu·∫≠t to√°n ##\n","\n","It's time to design a simple algorithm to distinguish cat images from non-cat images.\n","\n","C·∫•u tr√∫c c·ªßa thu·∫≠t to√°n Logistic Regression ƒë∆∞·ª£c m√¥ t·∫£ qua h√¨nh d∆∞·ªõi.\n","<div>\n","<img src=\"attachment:LogReg_kiank.png\" width=\"500\"/>\n","</div>\n","\n","\n","\n","**C√°c c√¥ng th·ª©c to√°n c·ªßa thu·∫≠t to√°n**:\n","\n","V·ªõi m·ªói b·ª©c ·∫£nh (l√† m·ªôt vector c·ªôt) $x^{(i)}$:\n","$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n","$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n","$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n","\n","Cost ƒë∆∞·ª£c t√≠nh b·∫±ng c√°ch t√≠nh t·ªïng Loss c·ªßa t·∫•t c·∫£ b·ª©c ·∫£nh training:\n","$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6Y-wYhqpevAL"},"source":["## 4 - X√¢y d·ª±ng c√°c th√†nh ph·∫ßn c·ªßa thu·∫≠t to√°n ## \n","\n","C√°c b∆∞·ªõc ch√≠nh ƒë·ªÉ x√¢y d·ª±ng m·ªôt Neural Network l√†:\n","1. X√°c ƒë·ªãnh c·∫•u tr√∫c c·ªßa m√¥ h√¨nh ( v√≠ d·ª• s·ªë l∆∞·ª£ng features c·ªßa d·ªØ li·ªáu, ·ªü ƒë√¢y l√† k√≠ch th∆∞·ªõc c·ªßa m·ªói b·ª©c ·∫£nh (vector c·ªôt)).\n","2. Kh·ªüi t·∫°o c√°c th√¥ng s·ªë c·ªßa m√¥ h√¨nh\n","3. Loop:\n","    - Calculate current loss (forward propagation)\n","    - Calculate current gradient (backward propagation)\n","    - Update parameters (gradient descent)\n","\n","C√°c b∆∞·ªõc 1-3 s·∫Ω ƒë∆∞·ª£c c√†i ƒë·∫∑t qua c√°c h√†m ri√™ng bi·ªát, sau ƒë√≥ ta s·∫Ω l·∫Øp gh√©p ch√∫ng l·∫°i ƒë·ªÉ t·∫°o th√†nh m√¥ h√¨nh 'model()'.\n","\n","\n","### 4.1 - C√†i ƒë·∫∑t sigmoid activation function\n","\n","B·∫Øt ƒë·∫ßu v√†o b√†i t·∫≠p nh√©! üòÖüòÖüòÖ\n","\n","B√†i t·∫≠p 1: H√£y c√†i ƒë·∫∑t h√†m sigmoid theo c√¥ng th·ª©c d∆∞·ªõi ƒë√¢y: \n","\n","$$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$ \n","\n","D√πng h√†m np.exp(z) trong python "]},{"cell_type":"code","metadata":{"id":"BVR6Fn4zevAL"},"source":["\n","def sigmoid(z):\n","    \"\"\"\n","    Compute the sigmoid of z\n","\n","    Arguments:\n","    z -- A scalar or numpy array of any size.\n","\n","    Return:\n","    a -- sigmoid(z)\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","    a = \n","    ### END CODE HERE ###\n","    \n","    return a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Plyqn9aqevAM"},"source":["print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wIV3n75_evAN"},"source":["**Expected Output**: \n","\n","<table>\n","  <tr>\n","    <td>**sigmoid([0, 2])**</td>\n","    <td> [ 0.5         0.88079708]</td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"8dwV17EpevAP"},"source":["### 4.2 - Kh·ªüi t·∫°o parameters\n","\n","B√†i t·∫≠p 2: Vi·∫øt h√†m kh·ªüi t·∫°o vector c·ªôt w v√† bias b.\n"]},{"cell_type":"code","metadata":{"id":"3H7CR6IcevAP"},"source":["#FUNCTION: initialize_with_zeros\n","\n","def initialize_with_zeros(dim):\n","    \"\"\"\n","    H√†m n√†y s·∫Ω kh·ªüi t·∫°o m·ªôt vector c·ªôt w to√†n zero k√≠ch th∆∞·ªõc (dim, 1) v√† kh·ªüi t·∫°o b b·∫±ng 0.\n","    \n","    Argument:\n","    dim -- k√≠ch th∆∞·ªõc vector c·ªôt w \n","    \n","    Returns:\n","    w -- vector c·ªôt kh·ªüi t·∫°o v·ªõi k√≠ch th∆∞·ªõc (dim, 1)\n","    b -- gi√° tr·ªã bias kh·ªüi t·∫°o k√≠ch th∆∞·ªõc (1, 1) \n","    \"\"\"\n","    \n","    ### START CODE HERE ###\n","    w = \n","    b = \n","    ### END CODE HERE ###\n","\n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    \n","    return w, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4KZhKRZevAQ"},"source":["dim = 2\n","w, b = initialize_with_zeros(dim)\n","print (\"w = \" + str(w))\n","print (\"b = \" + str(b))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"94KImk6gevAQ"},"source":["**Expected Output**: \n","\n","\n","<table style=\"width:15%\">\n","    <tr>\n","        <td>  ** w **  </td>\n","        <td> [[ 0.]\n"," [ 0.]] </td>\n","    </tr>\n","    <tr>\n","        <td>  ** b **  </td>\n","        <td> 0 </td>\n","    </tr>\n","</table>\n","\n","Khi ƒë·∫ßu v√†o l√† b·ª©c ·∫£nh, w s·∫Ω l√† vector c·ªôt k√≠ch th∆∞·ªõc (num_px $\\times$ num_px $\\times$ 3, 1)."]},{"cell_type":"markdown","metadata":{"id":"xRfXP9d7evAR"},"source":["### 4.3 - Forward and Backward propagation\n","\n","B√†i t·∫≠p 3: Vi·∫øt h√†m forward v√† backward propagation \n","\n","\n","\n","Forward Propagation:\n","- ƒê·∫ßu v√†o X, w, b\n","- T√≠nh $A = \\sigma(w^T X + b)$\n","- T√≠nh cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n","\n","Backward Propagation:\n","\n","- ƒê·∫ßu v√†o A, Y\n","- T√≠nh $dw = \\frac{1}{m}X(A-Y)^T $\n","- T√≠nh $db = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)})$\n","\n","Nh·∫Øc l·∫°i  &nbsp;&nbsp;    $ dw = \\frac{\\partial J}{\\partial w} $  &nbsp;&nbsp;     $ db = \\frac{\\partial J}{\\partial b} $\n","\n","D√πng h√†m np.log(), np.dot() trong python."]},{"cell_type":"code","metadata":{"id":"kraBVo1uevAR"},"source":["#FUNCTION: propagate\n","\n","def propagate(w, b, X, Y):\n","    \"\"\"\n","    H√†m n√†y s·∫Ω t√≠nh gi√° tr·ªã c·ªßa cost function (chi·ªÅu forward) v√† sau ƒë√≥ t√≠nh gradient c·ªßa c√°c th√¥ng s·ªë (chi·ªÅu backward)\n","\n","    Arguments:\n","    w -- vector c·ªôt weights k√≠ch th∆∞·ªõc (num_px * num_px * 3, 1)\n","    b -- bias (l√† m·ªôt s·ªë th·ª±c)\n","    X -- ma tr·∫≠n d·ªØ li·ªáu ƒë·∫ßu v√†o v·ªõi k√≠ch th∆∞·ªõc (num_px * num_px * 3, s·ªë l∆∞·ª£ng ·∫£nh training)\n","    Y -- vector h√†ng ch·ª©a c√°c nh√£n th·∫≠t (0 n·∫øu non-cat, 1 n·∫øu cat) v·ªõi k√≠ch th∆∞·ªõc (1, number of examples)\n","\n","    Return:\n","    cost -- negative log-likelihood cost \n","    grads -- t·ª´ ƒëi·ªÉn ch·ª©a gi√° tr·ªã dw v√† db v·ªõi\n","    dw -- gradient c·ªßa w, do ƒë√≥ dw c√≥ c√πng k√≠ch th∆∞·ªõc v·ªõi w\n","    db -- gradient c·ªßa b, do ƒë√≥ db c√≥ c√πng k√≠ch th∆∞·ªõc v·ªõi b\n","    \n","\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    # FORWARD PROPAGATION (t·ª´ X, w, b t√≠nh A. Sau ƒë√≥ t·ª´ A v√† Y t√≠nh cost)\n","    ### START CODE HERE ### \n","    A =                                # compute activation\n","    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * (np.log(1-A)))  # compute cost\n","    ### END CODE HERE ###\n","    \n","    # BACKWARD PROPAGATION (t√≠nh dz, dw, db)\n","    ### START CODE HERE ### \n","    dw = \n","    db = \n","    ### END CODE HERE ###\n","\n","    assert(dw.shape == w.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MG10Jc5uevAS"},"source":["w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n","grads, cost = propagate(w, b, X, Y)\n","print (\"dw = \" + str(grads[\"dw\"]))\n","print (\"db = \" + str(grads[\"db\"]))\n","print (\"cost = \" + str(cost))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0sit0_qvevAS"},"source":["**Expected Output**:\n","\n","<table style=\"width:50%\">\n","    <tr>\n","        <td>  ** dw **  </td>\n","      <td> [[ 0.99845601]\n","     [ 2.39507239]]</td>\n","    </tr>\n","    <tr>\n","        <td>  ** db **  </td>\n","        <td> 0.00145557813678 </td>\n","    </tr>\n","    <tr>\n","        <td>  ** cost **  </td>\n","        <td> 5.801545319394553 </td>\n","    </tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"5vQiYSnMevAT"},"source":["### 4.4 - Optimization s·ª≠ d·ª•ng gradient descent\n","\n","B√†i t·∫≠p 4: Vi·∫øt h√†m optimization."]},{"cell_type":"code","metadata":{"id":"b3Cxx8owevAT"},"source":["# FUNCTION: optimize\n","\n","def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n","    \"\"\"\n","    H√†m n√†y s·∫Ω optimizes w v√† b s·ª≠ d·ª•ng gradient descent\n","    \n","    Arguments:\n","    w -- vector c·ªôt weights k√≠ch th∆∞·ªõc (num_px * num_px * 3, 1)\n","    b -- bias (l√† m·ªôt s·ªë th·ª±c)\n","    X -- ma tr·∫≠n d·ªØ li·ªáu training k√≠ch th∆∞·ªõc (num_px * num_px * 3, s·ªë l∆∞·ª£ng ·∫£nh training)\n","    Y -- vector h√†ng ch·ª©a c√°c nh√£n ƒë√∫ng (0 n·∫øu non-cat, 1 n·∫øu cat) v·ªõi k√≠ch th∆∞·ªõc (1, s·ªë l∆∞·ª£ng ·∫£nh training)\n","    num_iterations -- s·ªë v√≤ng l·∫∑p cho optimization\n","    learning_rate -- learning rate\n","    print_cost -- True ƒë·ªÉ in ra gi√° tr·ªã loss ·ªü m·ªói 100 b∆∞·ªõc l·∫∑p\n","    \n","    Returns:\n","    params -- dictionary ch·ª©a w v√† b\n","    grads -- dictionary ch·ª©a dw v√† db\n","    costs -- list c√°c gi√° tr·ªã cost thu ƒë∆∞·ª£c trong qu√° tr√¨nh ch·∫°y (d√πng ƒë·ªÉ plot learning curve).\n","    \n","\n","    \"\"\"\n","    \n","    costs = []\n","    \n","    for i in range(num_iterations):\n","        \n","        \n","        # T√≠nh Cost and gradient d√πng h√†m propagate\n","        ### START CODE HERE ### \n","        grads, cost = propagate(w, b, X, Y)\n","        ### END CODE HERE ###\n","        \n","        # L·∫•y gi√° tr·ªã dw v√† db t·ª´ grads\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        \n","        # update l·∫°i c√°c gi√° tr·ªã w v√† b\n","        ### START CODE HERE ###\n","        w = \n","        b = \n","        ### END CODE HERE ###\n","        \n","        # L∆∞u l·∫°i gi√° tr·ªã costs\n","        if i % 100 == 0:\n","            costs.append(cost)\n","        \n","        # In ra cost m·ªói 100 l·∫ßn l·∫∑p\n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfsFWCpaevAT"},"source":["### 4.5 - H√†m predict\n","\n","V·ªõi th√¥ng s·ªë w v√† b ƒë√£ ƒë∆∞·ª£c trained, ta c√≥ th·ªÉ d·ª± ƒëo√°n nh√£n c·ªßa m·ªôt ho·∫∑c nhi·ªÅu t·∫•m ·∫£nh ƒë·∫ßu v√†o.\n","\n","C√≥ 2 b∆∞·ªõc c·∫ßn ph·∫£i th·ª±c hi·ªán trong h√†m predict(): \n","\n","1. T√≠nh gi√° tr·ªã activation $\\hat{Y} = A = \\sigma(w^T X + b)$ (l√† m·ªôt vector h√†ng g·ªìm con s·ªë trong kho·∫£ng (0,1))\n","\n","2. Chuy·ªÉn gi√° tr·ªã activation $\\hat{Y}$ ·ªü tr√™n th√†nh nh√£n 0 (n·∫øu activation <= 0.5) or 1 (n·∫øu activation > 0.5)."]},{"cell_type":"code","metadata":{"id":"o8OhpKcyevAU"},"source":["# FUNCTION: predict\n","\n","def predict(w, b, X):\n","    '''\n","    D·ª± ƒëo√°n nh√£n 0 hay 1 c·ªßa d·ªØ li·ªáu ƒë·∫ßu v√†o X s·ª≠ d·ª•ng (w, b)\n","    \n","    Arguments:\n","    w -- weights, k√≠ch th∆∞·ªõc (num_px * num_px * 3, 1)\n","    b -- bias (m·ªôt s·ªë th·ª±c)\n","    X -- d·ªØ li·ªáu ƒë·∫ßu v√†o k√≠ch th∆∞·ªõc (num_px * num_px * 3, s·ªë l∆∞·ª£ng ·∫£nh)\n","    \n","    Returns:\n","    Y_prediction -- m·ªôt vector h√†ng ch·ª©a t·∫•t c·∫£ nh√£n d·ª± ƒëo√°n c·ªßa c√°c t·∫•m ·∫£nh trong X\n","    '''\n","    \n","    m = X.shape[1]\n","    Y_prediction = np.zeros((1,m))\n","    w = w.reshape(X.shape[0], 1)\n","    \n","    # T√≠nh gi√° tr·ªã activation ·ª©ng v·ªõi c√°c t·∫•m ·∫£nh trong X\n","    A = sigmoid(np.dot(w.T, X) + b)\n","    \n","    \n","        \n","    # Chuy·ªÉn c√°c gi√° tr·ªã activation c√≥ gi√° tr·ªã trong kho·∫£ng (0,1) sang nh√£n (0 ho·∫∑c 1)    \n","    Y_prediction = 1. * (A > 0.5)\n","\n","    \n","    assert(Y_prediction.shape == (1, m))\n","    \n","    return Y_prediction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVBo0JP5evAU"},"source":["w = np.array([[0.1124579],[0.23106775]])\n","b = -0.3\n","X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n","print (\"predictions = \" + str(predict(w, b, X)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQpQwX8sevAV"},"source":["**Expected Output**: \n","\n","<table style=\"width:30%\">\n","    <tr>\n","         <td>\n","             **predictions**\n","         </td>\n","          <td>\n","            [[ 1.  1.  0.]]\n","         </td>  \n","   </tr>\n","\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"p6lGwAu3evAV"},"source":["## 5 - K·∫øt h·ª£p t·∫•t c·∫£ c√°c h√†m ƒë·ªÉ t·∫°o th√†nh m√¥ h√¨nh d·ª± ƒëo√°n ##\n","\n","You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n","\n","**Exercise:** Implement the model function. Use the following notation:\n","    - Y_prediction for your predictions on the test set\n","    - Y_prediction_train for your predictions on the train set\n","    - w, costs, grads for the outputs of optimize()"]},{"cell_type":"code","metadata":{"id":"8sR-6kVPevAV"},"source":["# GRADED FUNCTION: model\n","\n","def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n","    \"\"\"\n","    Builds the logistic regression model by calling the function you've implemented previously\n","    \n","    Arguments:\n","    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n","    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n","    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n","    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n","    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n","    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n","    print_cost -- Set to true to print the cost every 100 iterations\n","    \n","    Returns:\n","    d -- dictionary containing information about the model.\n","    \"\"\"\n","    \n","    \n","    # kh·ªüi t·∫°o\n","    w, b = initialize_with_zeros(X_train.shape[0])\n","\n","    # optimize\n","    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n","    \n","    # l·∫•y ra gi√° tr·ªã th√¥ng s·ªë w v√† b t·ª´ dictionary \"parameters\"\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","    \n","    # Test th·ª≠ ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh (v·ªõi w v√† b v·ª´a hu·∫•n luy·ªán ƒë∆∞·ª£c) tr√™n t·∫≠p train v√† t·∫≠p test \n","    Y_prediction_test = predict(w,b,X_test)\n","    Y_prediction_train = predict(w,b,X_train)\n","\n","\n","\n","    # Print train/test Errors\n","    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n","\n","    \n","    d = {\"costs\": costs,\n","         \"Y_prediction_test\": Y_prediction_test, \n","         \"Y_prediction_train\" : Y_prediction_train, \n","         \"w\" : w, \n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iterations\": num_iterations}\n","    \n","    return d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZOLkXdeevAW"},"source":["ƒê·∫øn l√∫c th·ª≠ nghi·ªám m√¥ h√¨nh r·ªìi üòª"]},{"cell_type":"code","metadata":{"id":"2nB5IIXtevAW"},"source":["d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeVMiDutevAX"},"source":["## 6 - Ph√¢n t√≠ch k·∫øt qu·∫£ ##\n","\n","Th·ª≠ plot cost function gi·∫£m th·∫ø n√†o qua c√°c b∆∞·ªõc l·∫∑p."]},{"cell_type":"code","metadata":{"id":"qnpmQincevAX"},"source":["# Plot learning curve (with costs)\n","costs = np.squeeze(d['costs'])\n","x=((np.arange(len(costs))+1)*100).tolist()\n","y=costs.tolist()\n","plt.plot(x,y)\n","plt.ylabel('cost')\n","plt.xlabel('S·ªë l·∫ßn l·∫∑p')\n","plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ToK03jXevAY"},"source":["V·∫´n c√≤n m·ªôt b√†i t·∫≠p n·ªØa üòÇüòÇüòÇ\n","\n","B√†i t·∫≠p 5: \n","\n","Ta th·∫•y cost function gi·∫£m theo s·ªë l·∫ßn l·∫∑p. Khi ch·∫°y tr√™n t·∫≠p train, ƒë·ªô ch√≠nh x√°c s·∫Ω gi·∫£m theo s·ªë l·∫ßn l·∫∑p. ƒêi·ªÅu n√†y ch·ª©ng t·ªè m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c. Nh∆∞ng m√¥ h√¨nh hi·ªán t·∫°i c√≥ d·∫•u hi·ªáu overfitting (ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p test th·∫•p h∆°n nhi·ªÅu tr√™n t·∫≠p train). H√£y ki·ªÉm ch·ª©ng ƒëi·ªÅu ƒë√≥ b·∫±ng c√°ch th·ª≠ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi s·ªë l·∫ßn l·∫∑p (num_iterations) kh√°c nhau (1000, 2000, 3000, 4000) v√† v·∫Ω bi·ªÉu ƒë·ªì hi·ªÉn th·ªã ƒë·ªô ch√≠nh x√°c train_accuracy v√† test_accuracy ·ª©ng v·ªõi c√°c s·ªë l·∫ßn l·∫∑p n√†y."]}]}